{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daviszars/nlp_cv_parser/blob/main/nlp_cv_parsing.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "CAEVyA4jc6MI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7BGMs0IDQGr"
      },
      "source": [
        "Darbam ar PDF dokumentiem tiek ienistalēta speciāla bibliotēka."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV NLP Model"
      ],
      "metadata": {
        "id": "b79omcavb4Wn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY45zyVdC-EV",
        "outputId": "1fd68c81-39b0-4ce1-d33c-4430cda34ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyacBm1QTuUz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from spacy.training import Example, offsets_to_biluo_tags\n",
        "from spacy.tokens import DocBin, span\n",
        "from spacy.util import minibatch, compounding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qGicWkNNg2Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download our pre-trained models and data"
      ],
      "metadata": {
        "id": "c-fpcFemf1Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/daviszars/nlp_cv_parser.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ObO_9eAG1p-",
        "outputId": "610b5810-682f-48d0-b725-5566e51e21f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp_cv_parser'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 32 (delta 4), reused 32 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (32/32), 7.42 MiB | 18.10 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DesCnPHPSQ07"
      },
      "source": [
        "NER model training / preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gPNqV802Ixi"
      },
      "outputs": [],
      "source": [
        "def align_entities_with_tokens(nlp, text, entities):\n",
        "    doc = nlp.make_doc(text)\n",
        "    valid_entities = []\n",
        "    for start, end, label in entities:\n",
        "        span = doc.char_span(start, end, alignment_mode=\"contract\")\n",
        "        if span is not None:\n",
        "            valid_entities.append((span.start_char, span.end_char, label))\n",
        "    return valid_entities\n",
        "\n",
        "def preprocess_training_data(train_data, nlp):\n",
        "    processed_data = []\n",
        "    for text, annotations in train_data:\n",
        "        entities = annotations['entities']\n",
        "        aligned_entities = align_entities_with_tokens(nlp, text, entities)\n",
        "        if aligned_entities:\n",
        "            processed_data.append((text, {'entities': aligned_entities}))\n",
        "        else:\n",
        "            print(f\"Skipping misaligned entity: {entities} in text: {text[:50]}\")\n",
        "    return processed_data\n",
        "\n",
        "def train_model(train_data, iterations=10):\n",
        "    nlp = spacy.blank(\"en\")  # blank English model\n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.add_pipe('ner', last=True)\n",
        "\n",
        "    # we add labels to the NER pipeline\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations['entities']:\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "\n",
        "    train_data = preprocess_training_data(train_data, nlp)\n",
        "\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train the NER pipeline\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(iterations):\n",
        "            print(f\"Iteration {itn + 1}/{iterations}\")\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
        "                try:\n",
        "                    nlp.update(examples, drop=0.5, sgd=optimizer, losses=losses)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error updating with batch: {batch}\")\n",
        "                    print(e)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    return nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDp17y_LSuYq"
      },
      "source": [
        "Testing / Validating misalignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YXKodg4C8nrg"
      },
      "outputs": [],
      "source": [
        "'''file_path = \"nlp_cv_parser/train_data.json\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    aligned_train_data = json.load(file)\n",
        "\n",
        "# Inspect the first few entries\n",
        "for i, entry in enumerate(aligned_train_data[:5]):  # Display first 5 entries\n",
        "    text, annotations = entry\n",
        "    print(f\"Entry {i + 1}:\")\n",
        "    print(f\"Text: {text[:100]}...\")  # Display only first 100 characters\n",
        "    print(f\"Annotations: {annotations}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def debug_alignment(data):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    misaligned_entries = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        biluo_tags = offsets_to_biluo_tags(doc, entities)\n",
        "\n",
        "        if '-' in biluo_tags:\n",
        "            misaligned_entries.append({\n",
        "                'text': text,\n",
        "                'entities': entities,\n",
        "                'tags': biluo_tags\n",
        "            })\n",
        "\n",
        "    return misaligned_entries\n",
        "\n",
        "misaligned_entries = debug_alignment(aligned_train_data)\n",
        "for entry in misaligned_entries\n",
        "    print(f\"Text: {entry['text']}\")\n",
        "    print(f\"Entities: {entry['entities']}\")\n",
        "    print(f\"BILUO Tags: {entry['tags']}\") # \"-\" is misaligned\n",
        "    print(\"\\n\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wzxIgh2Syz5"
      },
      "source": [
        "Training / saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5ybxAs3DT6-T",
        "outputId": "2df5f052-8b80-4eb3-b1ab-5e732bc2b164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/100\n",
            "Losses {'ner': 21999.84887876154}\n",
            "Iteration 2/100\n",
            "Losses {'ner': 3730.1003680919875}\n",
            "Iteration 3/100\n",
            "Losses {'ner': 3546.363207352472}\n",
            "Iteration 4/100\n",
            "Losses {'ner': 3647.454556387455}\n",
            "Iteration 5/100\n",
            "Losses {'ner': 3504.937798037518}\n",
            "Iteration 6/100\n",
            "Losses {'ner': 3696.9023374014246}\n",
            "Iteration 7/100\n",
            "Losses {'ner': 2945.993383268496}\n",
            "Iteration 8/100\n",
            "Losses {'ner': 2792.722916814341}\n",
            "Iteration 9/100\n",
            "Losses {'ner': 2907.0753568093514}\n",
            "Iteration 10/100\n",
            "Losses {'ner': 2563.8826700847103}\n",
            "Iteration 11/100\n",
            "Losses {'ner': 2723.5998962630447}\n",
            "Iteration 12/100\n",
            "Losses {'ner': 3042.909618486358}\n",
            "Iteration 13/100\n",
            "Losses {'ner': 2560.241305627979}\n",
            "Iteration 14/100\n",
            "Losses {'ner': 2594.31771974024}\n",
            "Iteration 15/100\n",
            "Losses {'ner': 2526.9499360683217}\n",
            "Iteration 16/100\n",
            "Losses {'ner': 2445.042784533822}\n",
            "Iteration 17/100\n",
            "Losses {'ner': 2294.8190113108258}\n",
            "Iteration 18/100\n",
            "Losses {'ner': 2308.2591326358606}\n",
            "Iteration 19/100\n",
            "Losses {'ner': 2401.826136999996}\n",
            "Iteration 20/100\n",
            "Losses {'ner': 2134.6318473807623}\n",
            "Iteration 21/100\n",
            "Losses {'ner': 2248.1215234360466}\n",
            "Iteration 22/100\n",
            "Losses {'ner': 2093.867791343917}\n",
            "Iteration 23/100\n",
            "Losses {'ner': 2129.6525471518075}\n",
            "Iteration 24/100\n",
            "Losses {'ner': 2240.6231101082108}\n",
            "Iteration 25/100\n",
            "Losses {'ner': 2151.107288554977}\n",
            "Iteration 26/100\n",
            "Losses {'ner': 2066.136766685125}\n",
            "Iteration 27/100\n",
            "Losses {'ner': 2217.8805292527722}\n",
            "Iteration 28/100\n",
            "Losses {'ner': 2115.8948983657287}\n",
            "Iteration 29/100\n",
            "Losses {'ner': 2061.5125652598235}\n",
            "Iteration 30/100\n",
            "Losses {'ner': 2403.247154840782}\n",
            "Iteration 31/100\n",
            "Losses {'ner': 2057.4211183810576}\n",
            "Iteration 32/100\n",
            "Losses {'ner': 1959.1406434118956}\n",
            "Iteration 33/100\n",
            "Losses {'ner': 2033.3650078321657}\n",
            "Iteration 34/100\n",
            "Losses {'ner': 1935.6853213602556}\n",
            "Iteration 35/100\n",
            "Losses {'ner': 1793.2520822315814}\n",
            "Iteration 36/100\n",
            "Losses {'ner': 1906.1780320606815}\n",
            "Iteration 37/100\n",
            "Losses {'ner': 1940.6736503014984}\n",
            "Iteration 38/100\n",
            "Losses {'ner': 1825.9130935172143}\n",
            "Iteration 39/100\n",
            "Losses {'ner': 1749.9568227606007}\n",
            "Iteration 40/100\n",
            "Losses {'ner': 1896.1827711353774}\n",
            "Iteration 41/100\n",
            "Losses {'ner': 1733.5175500742425}\n",
            "Iteration 42/100\n",
            "Losses {'ner': 1761.5547260079263}\n",
            "Iteration 43/100\n",
            "Losses {'ner': 1672.3987193008088}\n",
            "Iteration 44/100\n",
            "Losses {'ner': 1672.3857891190464}\n",
            "Iteration 45/100\n",
            "Losses {'ner': 1874.0354963196014}\n",
            "Iteration 46/100\n",
            "Losses {'ner': 1698.7983573010176}\n",
            "Iteration 47/100\n",
            "Losses {'ner': 1647.3056071819306}\n",
            "Iteration 48/100\n",
            "Losses {'ner': 1613.5609554616522}\n",
            "Iteration 49/100\n",
            "Losses {'ner': 1563.2218278980295}\n",
            "Iteration 50/100\n",
            "Losses {'ner': 1568.753947250706}\n",
            "Iteration 51/100\n",
            "Losses {'ner': 1611.5560364159983}\n",
            "Iteration 52/100\n",
            "Losses {'ner': 1535.1698141480024}\n",
            "Iteration 53/100\n",
            "Losses {'ner': 1623.7812333038582}\n",
            "Iteration 54/100\n",
            "Losses {'ner': 1552.5095207929196}\n",
            "Iteration 55/100\n",
            "Losses {'ner': 1494.0454553081831}\n",
            "Iteration 56/100\n",
            "Losses {'ner': 1470.7191021192557}\n",
            "Iteration 57/100\n",
            "Losses {'ner': 1517.6962537139195}\n",
            "Iteration 58/100\n",
            "Losses {'ner': 1505.45357495284}\n",
            "Iteration 59/100\n",
            "Losses {'ner': 1522.5968518394395}\n",
            "Iteration 60/100\n",
            "Losses {'ner': 1529.7730517340208}\n",
            "Iteration 61/100\n",
            "Losses {'ner': 1427.5507504118032}\n",
            "Iteration 62/100\n",
            "Losses {'ner': 1511.9077247860291}\n",
            "Iteration 63/100\n",
            "Losses {'ner': 1547.0989956940275}\n",
            "Iteration 64/100\n",
            "Losses {'ner': 1486.373654524394}\n",
            "Iteration 65/100\n",
            "Losses {'ner': 1410.50761807435}\n",
            "Iteration 66/100\n",
            "Losses {'ner': 1403.287990260446}\n",
            "Iteration 67/100\n",
            "Losses {'ner': 1382.1797510719684}\n",
            "Iteration 68/100\n",
            "Losses {'ner': 1390.4235609820869}\n",
            "Iteration 69/100\n",
            "Losses {'ner': 1389.1368031149786}\n",
            "Iteration 70/100\n",
            "Losses {'ner': 1366.8870636957113}\n",
            "Iteration 71/100\n",
            "Losses {'ner': 1372.802028385509}\n",
            "Iteration 72/100\n",
            "Losses {'ner': 1366.5515809966178}\n",
            "Iteration 73/100\n",
            "Losses {'ner': 1317.9577739423607}\n",
            "Iteration 74/100\n",
            "Losses {'ner': 1274.9653339545605}\n",
            "Iteration 75/100\n",
            "Losses {'ner': 1334.4886261127938}\n",
            "Iteration 76/100\n",
            "Losses {'ner': 1242.435356725037}\n",
            "Iteration 77/100\n",
            "Losses {'ner': 1321.6094595025118}\n",
            "Iteration 78/100\n",
            "Losses {'ner': 1308.306992717327}\n",
            "Iteration 79/100\n",
            "Losses {'ner': 1327.7916520017916}\n",
            "Iteration 80/100\n",
            "Losses {'ner': 1263.5730011748785}\n",
            "Iteration 81/100\n",
            "Losses {'ner': 1260.1947599451628}\n",
            "Iteration 82/100\n",
            "Losses {'ner': 1294.3328749096372}\n",
            "Iteration 83/100\n",
            "Losses {'ner': 1258.004698489499}\n",
            "Iteration 84/100\n",
            "Losses {'ner': 1282.0355634050038}\n",
            "Iteration 85/100\n",
            "Losses {'ner': 1260.2766742486021}\n",
            "Iteration 86/100\n",
            "Losses {'ner': 1257.5107438645343}\n",
            "Iteration 87/100\n",
            "Losses {'ner': 1199.5958466107095}\n",
            "Iteration 88/100\n",
            "Losses {'ner': 1281.3921150651815}\n",
            "Iteration 89/100\n",
            "Losses {'ner': 1278.7480448742165}\n",
            "Iteration 90/100\n",
            "Losses {'ner': 1234.1784645237044}\n",
            "Iteration 91/100\n",
            "Losses {'ner': 1197.0524153537917}\n",
            "Iteration 92/100\n",
            "Losses {'ner': 1244.6067672650224}\n",
            "Iteration 93/100\n",
            "Losses {'ner': 1160.7990901634828}\n",
            "Iteration 94/100\n",
            "Losses {'ner': 1116.297999593345}\n",
            "Iteration 95/100\n",
            "Losses {'ner': 1093.7245432711802}\n",
            "Iteration 96/100\n",
            "Losses {'ner': 1133.2718837164139}\n",
            "Iteration 97/100\n",
            "Losses {'ner': 1174.0308611636208}\n",
            "Iteration 98/100\n",
            "Losses {'ner': 1221.0847907758969}\n",
            "Iteration 99/100\n",
            "Losses {'ner': 1150.0359263856226}\n",
            "Iteration 100/100\n",
            "Losses {'ner': 1100.1991820657825}\n"
          ]
        }
      ],
      "source": [
        "# Load the cleaned data\n",
        "file_path = \"nlp_cv_parser/train_data.json\" # pagaidām manuāli jāieliek colab, uz beigām varēšu github ielikt, lai būtu download links\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Train the model\n",
        "nlp = train_model(train_data, iterations=100)\n",
        "\n",
        "# Save the model\n",
        "nlp.to_disk(\"nlp_cv_parser/ner_model_spacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTweo4IgTJJf"
      },
      "source": [
        "Convert data from pdf's"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"./trained_nlp_41min\"\n",
        "nlp.to_disk(model_name)"
      ],
      "metadata": {
        "id": "qTVLeAfYTQnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract the trained model"
      ],
      "metadata": {
        "id": "QAorJuaXT4xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/trained_nlp_41min"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoeJihrETrx5",
        "outputId": "1dacc549-9232-46be-9b0e-134edf08e294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/trained_nlp_41min/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/config.cfg (deflated 59%)\n",
            "  adding: content/trained_nlp_41min/ner/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/ner/model (deflated 8%)\n",
            "  adding: content/trained_nlp_41min/ner/moves (deflated 75%)\n",
            "  adding: content/trained_nlp_41min/ner/cfg (deflated 33%)\n",
            "  adding: content/trained_nlp_41min/vocab/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/key2row (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/strings.json (deflated 75%)\n",
            "  adding: content/trained_nlp_41min/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/vectors (deflated 45%)\n",
            "  adding: content/trained_nlp_41min/meta.json (deflated 50%)\n",
            "  adding: content/trained_nlp_41min/tokenizer (deflated 81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhPzkgCaJ0uW"
      },
      "source": [
        "Usable for both, create more training data and for end testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwwTM13oTIuP",
        "outputId": "08599d65-215f-4aae-af60-1297bb901624",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEAN\n",
            "PRICE\n",
            "IT Consultant\n",
            "DETAILS\n",
            "ADDRESS\n",
            "1515 Pacific Ave\n",
            "Los Angeles, CA 90291\n",
            "United States\n",
            "PHONE\n",
            "3868683442\n",
            "EMAIL\n",
            "email@email.com\n",
            "PLACE OF BIRTH\n",
            "San Antonio\n",
            "DRIVING LICENSE\n",
            "Full\n",
            "LINKS\n",
            "LinkedIn\n",
            "Pinterest\n",
            "Resume Templates\n",
            "Build this template\n",
            "HOBBIES\n",
            "Angling, Sailing, Fly Fishing\n",
            "LANGUAGES\n",
            "English\n",
            "French\n",
            "PROFILE\n",
            "Personable IT Consultant with 5+ years of experience in a global \n",
            "technology firm. CompTIA A+ Certification. Scored the region leading \n",
            "QST rating based on internal reviews (97.86%). I am seeking to leverage \n",
            "solid technical skills and abilities to advance my career as the next IT \n",
            "consultant for Linsang Group.\n",
            "EMPLOYMENT HISTORY\n",
            "IT Consultant , Amazon\n",
            "Jacksonville\n",
            "Jan 2020 — Jun 2021\n",
            "Administered first-level MHE and PKMS support and under-provided \n",
            "SOPs to make appropriate corrections when necessary.\n",
            "•\n",
            "Researched and documented existing and new processes for IT \n",
            "Support Teams and interacted with business users and other IT \n",
            "groups to ascertain business requirements and design proposed \n",
            "system enhancements.\n",
            "•\n",
            "Communicated issues, resolutions, and the project status to IT \n",
            "management and user community and ensured the deadlines \n",
            "were met and quality was maximized.\n",
            "•\n",
            "Deployed, reset, configured, and replaced equipment as needed, \n",
            "such as CLI Terminals, Printers, Silex Printer boxes., CPUs and \n",
            "laptops.\n",
            "•\n",
            "Coached newly hired IT specialists on advanced technical \n",
            "procedures.\n",
            "IT Consultant, PWC\n",
            "Pengcheng\n",
            "Jan 2019 — Dec 2021\n",
            "Independent, a non-profit organization that provides a broad array \n",
            "of assessment, research, information, and program management \n",
            "solutions in the education and workforce development areas.\n",
            "•\n",
            "Identified software and hardware issues and listened to client \n",
            "concerns.\n",
            "•\n",
            "Encouraged timely and relevant upgrades for client products \n",
            "when necessary.\n",
            "•\n",
            "Devised a workable scheme to accomplish business objectives.\n",
            "•\n",
            "Scheduled and allocated project activities, identified tools, \n",
            "standards, and guidelines suitable for projects.\n",
            "•\n",
            "Provided risk management by monitoring project schedules.\n",
            "•\n",
            "Reported on a project’s status regularly through emails and weekly \n",
            "meetings; formally tracked problems and issues to closure.\n",
            "EDUCATION\n",
            "Bachelor of Science in Information Systems \n",
            "Management, Miami University\n",
            "Miami Beach\n",
            "Jan 2020 — Jun 2021\n",
            "•\n",
            "Relevant Coursework: Network Security, IT Project Management, \n",
            "Business Administration, Strategy & Operations, IT Innovation, \n",
            "Ethical Hacking, Database Management.\n",
            "COURSES\n",
            "Microsoft Certified Solutions Expert, \n",
            "Microsoft. Online.\n",
            "Jan 2020 — Jun 2021\n",
            "CCNA Routing and Switching, Cisco. Online. \n",
            "Jan 2019 — Aug 2019\n",
            "ACHIEVEMENTS\n",
            "•\n",
            "Identified a new parts-ordering solution which led to a reduced \n",
            "client wait time of 19% and an increase in client satisfaction by 41%\n",
            "•\n",
            "Assisted the IT director with administration applications, reducing \n",
            "the workload by 22%\n",
            "•\n",
            "Identified ticketing management solutions which led to a queue \n",
            "reduction of 21%\n",
            "•\n",
            "Assisted the IT manager as liaison to clients on software updates, \n",
            "reducing workload by over 52%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys, fitz\n",
        "fname = 'nlp_cv_parser/Berlin-Simple-Resume-Template.pdf'\n",
        "doc = fitz.open(fname)\n",
        "text = \"\"\n",
        "for page in range(doc.page_count):\n",
        "    page_content = doc.load_page(page)  # Load each page\n",
        "    page_text = page_content.get_text(\"text\")  # Extract text from the page\n",
        "    text += page_text  # Append the extracted text to the all_text string\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing NER model on test input data"
      ],
      "metadata": {
        "id": "sYxuDoCFugCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kty5XmubSUtp",
        "outputId": "45d57579-765f-417f-c573-e620b55ce312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities in the test input:\n",
            "\n",
            "SEAN (0, 4): NAME\n",
            "IT Consultant (11, 24): DESIGNATION\n",
            "1515 Pacific Ave\n",
            "Los Angeles, CA 90291\n",
            "United States (41, 93): LOCATION\n",
            "email@email.com (117, 132): EMAIL ADDRESS\n",
            "English (291, 298): SKILLS\n",
            "IT Consultant (325, 338): DESIGNATION\n",
            "5+ years (344, 352): YEARS OF EXPERIENCE\n",
            "CompTIA A+ Certification. (397, 422): DEGREE\n",
            "technical skills (529, 545): SKILLS\n",
            "IT Consultant (646, 659): DESIGNATION\n",
            "Amazon (662, 668): COMPANIES WORKED AT\n",
            "MHE (727, 730): SKILLS\n",
            "PKMS (735, 739): SKILLS\n",
            "SOPs (768, 772): SKILLS\n",
            "IT Consultant (1402, 1415): DESIGNATION\n",
            "PWC (1417, 1420): COMPANIES WORKED AT\n",
            "program management (1561, 1579): SKILLS\n",
            "Bachelor of Science in Information (2172, 2206): DEGREE\n",
            "Miami University (2228, 2244): COLLEGE NAME\n",
            "CCNA Routing and Switching (2530, 2556): SKILLS\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_cv_spacy\")\n",
        "\n",
        "'''\n",
        "test_input = \"\"\"\n",
        "Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\n",
        "\"\"\"\n",
        "'''\n",
        "# test_input = \"\"\"\n",
        "# Alice Clark  AI / Machine Learning    Delhi, India Email me on Indeed  •  20+ years of experience in data handling, design, and development  •  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to  data warehousing and business intelligence  •  Database: Experience in database designing, scalability, back-up and recovery, writing and  optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.  Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure,  Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake  analytics(U-SQL)  Willing to relocate anywhere    WORK EXPERIENCE  Software Engineer  Microsoft – Bangalore, Karnataka  January 2000 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping  online. Microsoft Rewards members can earn points when searching with Bing, browsing with  Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft  Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft  rewards website. Rewards live dashboards gives a live picture of usage world-wide and by  markets like US, Canada, Australia, new user registration count, top/bottom performing rewards  offers, orders stats and weekly trends of user activities, orders and new user registrations. the  PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.  Technology/Tools used    EDUCATION  Indian Institute of Technology – Mumbai  2001    SKILLS  Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance\n",
        "# \"\"\"\n",
        "test_input = text\n",
        "# Process the input string using the model\n",
        "doc = nlp(test_input)\n",
        "\n",
        "print(\"Entities in the test input:\\n\")\n",
        "with open(\"nlp_cv_parser/extracted_ner.txt\", 'w') as file:\n",
        "    for ent in doc.ents:\n",
        "      print(f\"{ent.text} ({ent.start_char}, {ent.end_char}): {ent.label_}\")\n",
        "\n",
        "      #\n",
        "      file.write(f\"{ent.text} ({ent.start_char}, {ent.end_char}): {ent.label_}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-0oAAsCMEBx",
        "outputId": "68ed19f4-00d9-4af0-f875-1076620df518",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nuthin', 'Kan', 'Mar', 'Development', 'it', 'e', 'is', 'college', 'Might', 'Nov', 'provides', 'Honest', 'Bengaluru', 'where', 'X', 'Non', 'Del', 'must', 'growth', 'Team', 'Ariz', 'had', 'Calif', 'does', 'Cos', 'Would', 'do', 'b', 'science', 'Indeed', 'might', 'Mac', 'Has', 'pm', 'ought', 'Dec', 'ä', 'these', 'Role', 'Tenn', 'Miss', 'Ga', 'Must', 'somethin', 'What', 'by', 'ü', 'r', 'an', 'When', 'Have', 'Computer', 'You', 'Polite', 'Database', 'language', 'O', 'co', 'doin', 'Networks', 'Mt', 'To', 'Less', 'to', 'user', 'Let', 'xDD', 'Prof', 'Also', 'Ai', 'Does', 'Minn', 'g', 'Jha', 'La', 'of', 'Mich', 'Not', 'Okla', 'w', 'cos', 'year', 'Va', 'would', 'as', 'Kendriya', 'Rep', 'Player', 'Was', 'Management', 'k', 'Email', 'XD', 'Things', 'Sha', 'C', 'Ought', 'Sep', 'd', 'will', 'Who', 'November', 'f', 'need', 'ways', 'c', 'June', 'm', 'cause', 'EDUCATION', 'you', 'was', 'F', 'in', 'y', 'Developing', 'Jr', 'Mont', 'Bangalore', 'input', 'Where', 'nt', 'Oracle', 'Working', 'Gen', 'given', 'the', 'company', 'INFORMATION', 'Bot', 'who', 'May', 'Operating', 'why', 'Are', 'how', 'Different', 'could', 'h', 'relocate', 'Hard', 'ca', 'cuz', 'Associate', 'Cuz', 'Havin', 'those', 'space', 'He', 'Neb', 'Nev', 'were', 't', 'ta', 'a', 'dare', 'PeopleSoft', 'Messrs', 'll', 'this', 'technology', 'Training', 'Were', 'Jan', 'organization', 'possible', 'nuff', 'they', 'We', 'my', 'he', 'Skills', 'Machine', 'Can', 'worked', 'modern', 'Jun', 'l', 'Calm', 'Lovin', 'work', 'bot', 'u', 'Ol', 'Dare', 'q', 'Internet', 'there', 'havin', 'Need', 'Somethin', 'gon', 'Ia', 'v', 'got', 'April', 'Wash', 'Conn', 'Mass', 'Colo', 'individual', 'It', 'j', 'Those', 'bout', 'Willing', 'Mr', 'Bros', 'Mo', 'Why', 'K', 'Wo', 'or', 'Flexible', 'when', 'which', 'Dr', 'Is', 'she', 'than', 'Id', 'Apr', 'Situations', 'may', 'all', 'Ltd', 'Doin', 'Fla', 'negative', 'what', 'Ind', 'Sept', 'Sen', 'coz', 'nothin', 'Java', 'Linux', 'Information', 'Application', 'Ark', 'Co', 'This', 'WORK', 'August', 'positive', 'different', 'Rev', 'Ms', 're', 'Gon', 'There', 'D', 'lovin', 'S', 'Mrs', 'Gov', 'Feb', 'me', 'for', 'Nuthin', 'Woodbine', 'Corp', 'These', 'Ky', 'improve', 'not', 'Could', 'That', 'did', 'Mathematics', 'I', 'Do', 'Backend', 'Wis', 'Technical', 'has', 'Inc', 'Ala', 'Of', 'March', 'have', 'She', 'sha', 'EXPERIENCE', 'Both', 'vs', 'Currently', 'Did', 's', 'p', 'best', 'am', 'Vidyalaya', 'ol', 'engineering', 'Karnataka', 'working', 'Pa', 'knowledge', 'Queries', 'can', 'Jul', 'Abhishek', 'ADDITIONAL', 'we', 'Accenture', 'n', 'em', 'on', 'SKILLS', 'Windows', 'Ph', 'x', 'Hubli', 'Ca', 'that', 'o', 'St', 'Should', 'school', 'Oct', 'Md', 'utterances', 'Kans', 'na', 'i', 'xD', 'Ak', 'based', 'opportunity', 'Nothin', 'z', 'Chat', 'How', 'triggered', 'They', 'ö', 'ai', 'Had', 'Got', 'Aug', 'XDD', 'P', 'Cause', 'skills', 'wo', 'Goin', 'Tolerant', 'should', 'Learning', 'Ill', 'are', 'Coz', 'Nebr', 'Programming', 'Adm', 'and', 'System', 'Ore', 'goin', 'let', 'Present', 've', 'be']\n"
          ]
        }
      ],
      "source": [
        "vocab = nlp.vocab\n",
        "\n",
        "# List all tokens in the vocabulary\n",
        "tokens = [word.text for word in vocab if word.is_alpha]\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pTX_7aHMqU9",
        "outputId": "8865dcda-2bb6-4745-98cb-035efbd696dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Labels:\n",
            "('COLLEGE NAME', 'COMPANIES WORKED AT', 'DEGREE', 'DESIGNATION', 'EMAIL ADDRESS', 'GRADUATION YEAR', 'LOCATION', 'NAME', 'SKILLS', 'UNKNOWN', 'YEARS OF EXPERIENCE')\n"
          ]
        }
      ],
      "source": [
        "# Get the entity labels\n",
        "ner_labels = nlp.get_pipe(\"ner\").labels\n",
        "\n",
        "print(\"NER Labels:\")\n",
        "print(ner_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW-hKLoSJYRH",
        "outputId": "06d965ad-a850-47dd-f30b-e278cb3dd1ac",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEAN 0-3\n",
            "PRICE 5-9\n",
            "IT 11-12\n",
            "Consultant 14-23\n",
            "DETAILS 25-31\n",
            "ADDRESS 33-39\n",
            "1515 41-44\n",
            "Pacific 46-52\n",
            "Ave 54-56\n",
            "Los 58-60\n",
            "Angeles, 62-69\n",
            "CA 71-72\n",
            "90291 74-78\n",
            "United 80-85\n",
            "States 87-92\n",
            "PHONE 94-98\n",
            "3868683442 100-109\n",
            "EMAIL 111-115\n",
            "email@email.com 117-131\n",
            "PLACE 133-137\n",
            "OF 139-140\n",
            "BIRTH 142-146\n",
            "San 148-150\n",
            "Antonio 152-158\n",
            "DRIVING 160-166\n",
            "LICENSE 168-174\n",
            "Full 176-179\n",
            "LINKS 181-185\n",
            "LinkedIn 187-194\n",
            "Pinterest 196-204\n",
            "Resume 206-211\n",
            "Templates 213-221\n",
            "Build 223-227\n",
            "this 229-232\n",
            "template 234-241\n",
            "HOBBIES 243-249\n",
            "Angling, 251-258\n",
            "Sailing, 260-267\n",
            "Fly 269-271\n",
            "Fishing 273-279\n",
            "LANGUAGES 281-289\n",
            "English 291-297\n",
            "French 299-304\n",
            "PROFILE 306-312\n",
            "Personable 314-323\n",
            "IT 325-326\n",
            "Consultant 328-337\n",
            "with 339-342\n",
            "5+ 344-345\n",
            "years 347-351\n",
            "of 353-354\n",
            "experience 356-365\n",
            "in 367-368\n",
            "a 370-370\n",
            "global 372-377\n",
            "technology 380-389\n",
            "firm. 391-395\n",
            "CompTIA 397-403\n",
            "A+ 405-406\n",
            "Certification. 408-421\n",
            "Scored 423-428\n",
            "the 430-432\n",
            "region 434-439\n",
            "leading 441-447\n",
            "QST 450-452\n",
            "rating 454-459\n",
            "based 461-465\n",
            "on 467-468\n",
            "internal 470-477\n",
            "reviews 479-485\n",
            "(97.86%). 487-495\n",
            "I 497-497\n",
            "am 499-500\n",
            "seeking 502-508\n",
            "to 510-511\n",
            "leverage 513-520\n",
            "solid 523-527\n",
            "technical 529-537\n",
            "skills 539-544\n",
            "and 546-548\n",
            "abilities 550-558\n",
            "to 560-561\n",
            "advance 563-569\n",
            "my 571-572\n",
            "career 574-579\n",
            "as 581-582\n",
            "the 584-586\n",
            "next 588-591\n",
            "IT 593-594\n",
            "consultant 597-606\n",
            "for 608-610\n",
            "Linsang 612-618\n",
            "Group. 620-625\n",
            "EMPLOYMENT 627-636\n",
            "HISTORY 638-644\n",
            "IT 646-647\n",
            "Consultant 649-658\n",
            ", 660-660\n",
            "Amazon 662-667\n",
            "Jacksonville 669-680\n",
            "Jan 682-684\n",
            "2020 686-689\n",
            "— 691-691\n",
            "Jun 693-695\n",
            "2021 697-700\n",
            "Administered 702-713\n",
            "first-level 715-725\n",
            "MHE 727-729\n",
            "and 731-733\n",
            "PKMS 735-738\n",
            "support 740-746\n",
            "and 748-750\n",
            "under-provided 752-765\n",
            "SOPs 768-771\n",
            "to 773-774\n",
            "make 776-779\n",
            "appropriate 781-791\n",
            "corrections 793-803\n",
            "when 805-808\n",
            "necessary. 810-819\n",
            "• 821-821\n",
            "Researched 823-832\n",
            "and 834-836\n",
            "documented 838-847\n",
            "existing 849-856\n",
            "and 858-860\n",
            "new 862-864\n",
            "processes 866-874\n",
            "for 876-878\n",
            "IT 880-881\n",
            "Support 884-890\n",
            "Teams 892-896\n",
            "and 898-900\n",
            "interacted 902-911\n",
            "with 913-916\n",
            "business 918-925\n",
            "users 927-931\n",
            "and 933-935\n",
            "other 937-941\n",
            "IT 943-944\n",
            "groups 947-952\n",
            "to 954-955\n",
            "ascertain 957-965\n",
            "business 967-974\n",
            "requirements 976-987\n",
            "and 989-991\n",
            "design 993-998\n",
            "proposed 1000-1007\n",
            "system 1010-1015\n",
            "enhancements. 1017-1029\n",
            "• 1031-1031\n",
            "Communicated 1033-1044\n",
            "issues, 1046-1052\n",
            "resolutions, 1054-1065\n",
            "and 1067-1069\n",
            "the 1071-1073\n",
            "project 1075-1081\n",
            "status 1083-1088\n",
            "to 1090-1091\n",
            "IT 1093-1094\n",
            "management 1097-1106\n",
            "and 1108-1110\n",
            "user 1112-1115\n",
            "community 1117-1125\n",
            "and 1127-1129\n",
            "ensured 1131-1137\n",
            "the 1139-1141\n",
            "deadlines 1143-1151\n",
            "were 1154-1157\n",
            "met 1159-1161\n",
            "and 1163-1165\n",
            "quality 1167-1173\n",
            "was 1175-1177\n",
            "maximized. 1179-1188\n",
            "• 1190-1190\n",
            "Deployed, 1192-1200\n",
            "reset, 1202-1207\n",
            "configured, 1209-1219\n",
            "and 1221-1223\n",
            "replaced 1225-1232\n",
            "equipment 1234-1242\n",
            "as 1244-1245\n",
            "needed, 1247-1253\n",
            "such 1256-1259\n",
            "as 1261-1262\n",
            "CLI 1264-1266\n",
            "Terminals, 1268-1277\n",
            "Printers, 1279-1287\n",
            "Silex 1289-1293\n",
            "Printer 1295-1301\n",
            "boxes., 1303-1309\n",
            "CPUs 1311-1314\n",
            "and 1316-1318\n",
            "laptops. 1321-1328\n",
            "• 1330-1330\n",
            "Coached 1332-1338\n",
            "newly 1340-1344\n",
            "hired 1346-1350\n",
            "IT 1352-1353\n",
            "specialists 1355-1365\n",
            "on 1367-1368\n",
            "advanced 1370-1377\n",
            "technical 1379-1387\n",
            "procedures. 1390-1400\n",
            "IT 1402-1403\n",
            "Consultant, 1405-1415\n",
            "PWC 1417-1419\n",
            "Pengcheng 1421-1429\n",
            "Jan 1431-1433\n",
            "2019 1435-1438\n",
            "— 1440-1440\n",
            "Dec 1442-1444\n",
            "2021 1446-1449\n",
            "Independent, 1451-1462\n",
            "a 1464-1464\n",
            "non-profit 1466-1475\n",
            "organization 1477-1488\n",
            "that 1490-1493\n",
            "provides 1495-1502\n",
            "a 1504-1504\n",
            "broad 1506-1510\n",
            "array 1512-1516\n",
            "of 1519-1520\n",
            "assessment, 1522-1532\n",
            "research, 1534-1542\n",
            "information, 1544-1555\n",
            "and 1557-1559\n",
            "program 1561-1567\n",
            "management 1569-1578\n",
            "solutions 1581-1589\n",
            "in 1591-1592\n",
            "the 1594-1596\n",
            "education 1598-1606\n",
            "and 1608-1610\n",
            "workforce 1612-1620\n",
            "development 1622-1632\n",
            "areas. 1634-1639\n",
            "• 1641-1641\n",
            "Identified 1643-1652\n",
            "software 1654-1661\n",
            "and 1663-1665\n",
            "hardware 1667-1674\n",
            "issues 1676-1681\n",
            "and 1683-1685\n",
            "listened 1687-1694\n",
            "to 1696-1697\n",
            "client 1699-1704\n",
            "concerns. 1707-1715\n",
            "• 1717-1717\n",
            "Encouraged 1719-1728\n",
            "timely 1730-1735\n",
            "and 1737-1739\n",
            "relevant 1741-1748\n",
            "upgrades 1750-1757\n",
            "for 1759-1761\n",
            "client 1763-1768\n",
            "products 1770-1777\n",
            "when 1780-1783\n",
            "necessary. 1785-1794\n",
            "• 1796-1796\n",
            "Devised 1798-1804\n",
            "a 1806-1806\n",
            "workable 1808-1815\n",
            "scheme 1817-1822\n",
            "to 1824-1825\n",
            "accomplish 1827-1836\n",
            "business 1838-1845\n",
            "objectives. 1847-1857\n",
            "• 1859-1859\n",
            "Scheduled 1861-1869\n",
            "and 1871-1873\n",
            "allocated 1875-1883\n",
            "project 1885-1891\n",
            "activities, 1893-1903\n",
            "identified 1905-1914\n",
            "tools, 1916-1921\n",
            "standards, 1924-1933\n",
            "and 1935-1937\n",
            "guidelines 1939-1948\n",
            "suitable 1950-1957\n",
            "for 1959-1961\n",
            "projects. 1963-1971\n",
            "• 1973-1973\n",
            "Provided 1975-1982\n",
            "risk 1984-1987\n",
            "management 1989-1998\n",
            "by 2000-2001\n",
            "monitoring 2003-2012\n",
            "project 2014-2020\n",
            "schedules. 2022-2031\n",
            "• 2033-2033\n",
            "Reported 2035-2042\n",
            "on 2044-2045\n",
            "a 2047-2047\n",
            "project’s 2049-2057\n",
            "status 2059-2064\n",
            "regularly 2066-2074\n",
            "through 2076-2082\n",
            "emails 2084-2089\n",
            "and 2091-2093\n",
            "weekly 2095-2100\n",
            "meetings; 2103-2111\n",
            "formally 2113-2120\n",
            "tracked 2122-2128\n",
            "problems 2130-2137\n",
            "and 2139-2141\n",
            "issues 2143-2148\n",
            "to 2150-2151\n",
            "closure. 2153-2160\n",
            "EDUCATION 2162-2170\n",
            "Bachelor 2172-2179\n",
            "of 2181-2182\n",
            "Science 2184-2190\n",
            "in 2192-2193\n",
            "Information 2195-2205\n",
            "Systems 2207-2213\n",
            "Management, 2216-2226\n",
            "Miami 2228-2232\n",
            "University 2234-2243\n",
            "Miami 2245-2249\n",
            "Beach 2251-2255\n",
            "Jan 2257-2259\n",
            "2020 2261-2264\n",
            "— 2266-2266\n",
            "Jun 2268-2270\n",
            "2021 2272-2275\n",
            "• 2277-2277\n",
            "Relevant 2279-2286\n",
            "Coursework: 2288-2298\n",
            "Network 2300-2306\n",
            "Security, 2308-2316\n",
            "IT 2318-2319\n",
            "Project 2321-2327\n",
            "Management, 2329-2339\n",
            "Business 2342-2349\n",
            "Administration, 2351-2365\n",
            "Strategy 2367-2374\n",
            "& 2376-2376\n",
            "Operations, 2378-2388\n",
            "IT 2390-2391\n",
            "Innovation, 2393-2403\n",
            "Ethical 2406-2412\n",
            "Hacking, 2414-2421\n",
            "Database 2423-2430\n",
            "Management. 2432-2442\n",
            "COURSES 2444-2450\n",
            "Microsoft 2452-2460\n",
            "Certified 2462-2470\n",
            "Solutions 2472-2480\n",
            "Expert, 2482-2488\n",
            "Microsoft. 2491-2500\n",
            "Online. 2502-2508\n",
            "Jan 2510-2512\n",
            "2020 2514-2517\n",
            "— 2519-2519\n",
            "Jun 2521-2523\n",
            "2021 2525-2528\n",
            "CCNA 2530-2533\n",
            "Routing 2535-2541\n",
            "and 2543-2545\n",
            "Switching, 2547-2556\n",
            "Cisco. 2558-2563\n",
            "Online. 2565-2571\n",
            "Jan 2574-2576\n",
            "2019 2578-2581\n",
            "— 2583-2583\n",
            "Aug 2585-2587\n",
            "2019 2589-2592\n",
            "ACHIEVEMENTS 2594-2605\n",
            "• 2607-2607\n",
            "Identified 2609-2618\n",
            "a 2620-2620\n",
            "new 2622-2624\n",
            "parts-ordering 2626-2639\n",
            "solution 2641-2648\n",
            "which 2650-2654\n",
            "led 2656-2658\n",
            "to 2660-2661\n",
            "a 2663-2663\n",
            "reduced 2665-2671\n",
            "client 2674-2679\n",
            "wait 2681-2684\n",
            "time 2686-2689\n",
            "of 2691-2692\n",
            "19% 2694-2696\n",
            "and 2698-2700\n",
            "an 2702-2703\n",
            "increase 2705-2712\n",
            "in 2714-2715\n",
            "client 2717-2722\n",
            "satisfaction 2724-2735\n",
            "by 2737-2738\n",
            "41% 2740-2742\n",
            "• 2744-2744\n",
            "Assisted 2746-2753\n",
            "the 2755-2757\n",
            "IT 2759-2760\n",
            "director 2762-2769\n",
            "with 2771-2774\n",
            "administration 2776-2789\n",
            "applications, 2791-2803\n",
            "reducing 2805-2812\n",
            "the 2815-2817\n",
            "workload 2819-2826\n",
            "by 2828-2829\n",
            "22% 2831-2833\n",
            "• 2835-2835\n",
            "Identified 2837-2846\n",
            "ticketing 2848-2856\n",
            "management 2858-2867\n",
            "solutions 2869-2877\n",
            "which 2879-2883\n",
            "led 2885-2887\n",
            "to 2889-2890\n",
            "a 2892-2892\n",
            "queue 2894-2898\n",
            "reduction 2901-2909\n",
            "of 2911-2912\n",
            "21% 2914-2916\n",
            "• 2918-2918\n",
            "Assisted 2920-2927\n",
            "the 2929-2931\n",
            "IT 2933-2934\n",
            "manager 2936-2942\n",
            "as 2944-2945\n",
            "liaison 2947-2953\n",
            "to 2955-2956\n",
            "clients 2958-2964\n",
            "on 2966-2967\n",
            "software 2969-2976\n",
            "updates, 2978-2985\n",
            "reducing 2988-2995\n",
            "workload 2997-3004\n",
            "by 3006-3007\n",
            "over 3009-3012\n",
            "52% 3014-3016\n"
          ]
        }
      ],
      "source": [
        "def print_word_locations(text):\n",
        "    words = text.split()\n",
        "    start_pos = 0\n",
        "\n",
        "    for word in words:\n",
        "        start_pos = text.find(word, start_pos)\n",
        "        end_pos = start_pos + len(word) - 1\n",
        "        print(f\"{word} {start_pos}-{end_pos}\")\n",
        "        start_pos += len(word)\n",
        "\n",
        "print_word_locations(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert sample data set to uppercase so the Ner-Anotator can be used."
      ],
      "metadata": {
        "id": "tUi9oezWGEHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra samples taken from: https://www.resumeviking.com/templates/\n",
        "Tagged with: https://tecoholic.github.io/ner-annotator/"
      ],
      "metadata": {
        "id": "Qb7wCxGgLyXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def convert_tags_in_json(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Ensure the data is a list of dictionaries with 'tag' key\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"JSON file should contain a list of dictionaries\")\n",
        "\n",
        "        # Convert tags to uppercase\n",
        "        for item in data:\n",
        "            for tag in range(len(item[1]['entities'])):\n",
        "              item[1]['entities'][tag][2] = item[1]['entities'][tag][2].upper()\n",
        "\n",
        "        # Write the updated JSON back to the file\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(\"Tags converted to uppercase successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "file_path = 'nlp_cv_parser/train_data.json'\n",
        "convert_tags_in_json(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpXJP70rC5v9",
        "outputId": "5277143f-f8fe-48e2-9eeb-06d14fc0541b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tags converted to uppercase successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Job listing NLP"
      ],
      "metadata": {
        "id": "Nzdf4u4ydM80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"nlp_cv_parser/train_data_jobs.json\"\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Train the model\n",
        "nlp_job = train_model(train_data, iterations=100)\n",
        "\n",
        "# Save the model\n",
        "nlp_job.to_disk(\"nlp_cv_parser/ner_model_spacy\")"
      ],
      "metadata": {
        "id": "rVProyYndPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jobs NER model test"
      ],
      "metadata": {
        "id": "km_VMlXDSz82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_jobs_spacy\")\n",
        "\n",
        "with open(\"nlp_cv_parser/test_data_jobs.json\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "def create_examples(data, nlp):\n",
        "    examples = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        spans = [(start, end, label) for start, end, label in entities]\n",
        "        example = Example.from_dict(doc, {\"entities\": spans})\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "test_examples = create_examples(test_data, nlp)\n",
        "\n",
        "def evaluate_model(nlp, examples):\n",
        "    scorer = nlp.evaluate(examples)\n",
        "    return scorer\n",
        "\n",
        "scorer = evaluate_model(nlp, test_examples)\n",
        "\n",
        "print(f\"Precision: {scorer['ents_p']}\")\n",
        "print(f\"Recall: {scorer['ents_r']}\")\n",
        "print(f\"F1-score: {scorer['ents_f']}\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Extract true and predicted entities with their labels\n",
        "for example in test_examples:\n",
        "    gold_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in example.reference.ents]\n",
        "    pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in nlp(example.text).ents]\n",
        "\n",
        "    gold_map = { (start, end): label for start, end, label in gold_ents }\n",
        "\n",
        "    pred_map = { (start, end): label for start, end, label in pred_ents }\n",
        "\n",
        "    all_positions = set(gold_map.keys()).union(set(pred_map.keys()))\n",
        "\n",
        "    # Populate y_true and y_pred based on positions\n",
        "    for pos in all_positions:\n",
        "        y_true.append(gold_map.get(pos, 'O'))\n",
        "        y_pred.append(pred_map.get(pos, 'O'))\n",
        "\n",
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsj-uIPsq-cq",
        "outputId": "e1d5a34a-7cd2-4fc6-a09a-d8bb33e8ec98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3425925925925926\n",
            "Recall: 0.2813688212927757\n",
            "F1-score: 0.30897703549060546\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "      EXPERIENCE       0.37      0.39      0.38        36\n",
            "               O       0.00      0.00      0.00       124\n",
            "   QUALIFICATION       0.21      0.18      0.19        34\n",
            "    REQUIREMENTS       1.00      0.02      0.04        46\n",
            "RESPONSIBILITIES       0.42      0.38      0.40        97\n",
            "          SKILLS       0.27      0.32      0.29        50\n",
            "\n",
            "        accuracy                           0.19       387\n",
            "       macro avg       0.38      0.21      0.22       387\n",
            "    weighted avg       0.31      0.19      0.19       387\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV NER model test"
      ],
      "metadata": {
        "id": "1kLX_ogeSvM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_cv_spacy\")\n",
        "\n",
        "with open(\"nlp_cv_parser/test_data.json\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "def create_examples(data, nlp):\n",
        "    examples = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        spans = [(start, end, label) for start, end, label in entities]\n",
        "        example = Example.from_dict(doc, {\"entities\": spans})\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "test_examples = create_examples(test_data, nlp)\n",
        "\n",
        "def evaluate_model(nlp, examples):\n",
        "    scorer = nlp.evaluate(examples)\n",
        "    return scorer\n",
        "\n",
        "scorer = evaluate_model(nlp, test_examples)\n",
        "\n",
        "print(f\"Precision: {scorer['ents_p']}\")\n",
        "print(f\"Recall: {scorer['ents_r']}\")\n",
        "print(f\"F1-score: {scorer['ents_f']}\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Extract true and predicted entities with their labels\n",
        "for example in test_examples:\n",
        "    gold_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in example.reference.ents]\n",
        "    pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in nlp(example.text).ents]\n",
        "\n",
        "    gold_map = { (start, end): label for start, end, label in gold_ents }\n",
        "\n",
        "    pred_map = { (start, end): label for start, end, label in pred_ents }\n",
        "\n",
        "    all_positions = set(gold_map.keys()).union(set(pred_map.keys()))\n",
        "\n",
        "    # Populate y_true and y_pred based on positions\n",
        "    for pos in all_positions:\n",
        "        y_true.append(gold_map.get(pos, 'O'))\n",
        "        y_pred.append(pred_map.get(pos, 'O'))\n",
        "\n",
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOFg3DS5SLB1",
        "outputId": "cb9c908d-f038-4ec1-8cb4-e9e2ceee680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Abhishek Jha Application Development Associate - A...\" with entities \"[(0, 11, 'NAME'), (13, 45, 'DESIGNATION'), (49, 57...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Afreen Jamadar Active member of IIIT Committee in ...\" with entities \"[(0, 13, 'NAME'), (62, 67, 'LOCATION'), (104, 147,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Akhil Yadav Polemaina Hyderabad, Telangana - Email...\" with entities \"[(0, 20, 'NAME'), (22, 30, 'LOCATION'), (65, 116, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Alok Khandai Operational Analyst (SQL DBA) Enginee...\" with entities \"[(0, 11, 'NAME'), (54, 59, 'COMPANIES WORKED AT'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ananya Chavan lecturer - oracle tutorials  Mumbai,...\" with entities \"[(0, 12, 'NAME'), (14, 21, 'DESIGNATION'), (25, 40...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Anvitha Rao Automation developer  - Email me on In...\" with entities \"[(0, 10, 'NAME'), (12, 31, 'DESIGNATION'), (56, 96...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"arjun ks Senior Program coordinator - oracle India...\" with entities \"[(0, 7, 'NAME'), (9, 34, 'DESIGNATION'), (38, 43, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Arun Elumalai QA Tester  Chennai, Tamil Nadu - Ema...\" with entities \"[(0, 12, 'NAME'), (14, 23, 'DESIGNATION'), (25, 31...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ashalata Bisoyi Transaction Processor - Oracle Ind...\" with entities \"[(955, 990, 'DEGREE'), (993, 1021, 'COLLEGE NAME')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ashok Kunam Team Lead - Microsoft  - Email me on I...\" with entities \"[(0, 10, 'NAME'), (12, 20, 'DESIGNATION'), (24, 32...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Asish Ratha Subject matter Expert - Accenture  Che...\" with entities \"[(0, 10, 'NAME'), (12, 32, 'DESIGNATION'), (36, 44...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Avin Sharma Senior Associate Consultant - Infosys ...\" with entities \"[(0, 10, 'NAME'), (12, 39, 'DESIGNATION'), (42, 56...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ayesha B Team member - Oracle  Bangalore, Karnatak...\" with entities \"[(0, 6, 'NAME'), (9, 19, 'DESIGNATION'), (23, 28, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ayushi Srivastava Senior Analyst - Cisco  New Delh...\" with entities \"[(0, 16, 'NAME'), (42, 50, 'LOCATION'), (81, 127, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Bhawana Daf Pune, Maharashtra - Email me on Indeed...\" with entities \"[(0, 10, 'NAME'), (12, 15, 'LOCATION'), (52, 93, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Darshan G. Financial Analyst - Oracle  Bengaluru, ...\" with entities \"[(0, 9, 'NAME'), (11, 27, 'DESIGNATION'), (31, 36,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dhanushkodi Raj Technology Analyst - Infosys Limit...\" with entities \"[(0, 14, 'NAME'), (16, 33, 'DESIGNATION'), (37, 52...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dinesh Reddy Deployed chef for configuration manag...\" with entities \"[(0, 11, 'NAME'), (13, 69, 'DESIGNATION'), (73, 77...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dipesh Gulati Co-coordinator of CODE HUNTER at CGC...\" with entities \"[(0, 12, 'NAME'), (57, 61, 'LOCATION'), (64, 68, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dushyant Bhatt BI / Big Data/ Azure  Hyderabad-Dec...\" with entities \"[(0, 13, 'NAME'), (37, 45, 'LOCATION'), (729, 738,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Govardhana K Senior Software Engineer  Bengaluru, ...\" with entities \"[(0, 10, 'NAME'), (13, 37, 'DESIGNATION'), (39, 47...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9565217391304348\n",
            "Recall: 0.8825214899713467\n",
            "F1-score: 0.9180327868852459\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       COLLEGE NAME       0.45      0.88      0.60        26\n",
            "COMPANIES WORKED AT       0.70      0.54      0.61        35\n",
            "             DEGREE       0.67      0.90      0.77        31\n",
            "        DESIGNATION       0.60      0.89      0.72        53\n",
            "      EMAIL ADDRESS       0.75      1.00      0.86         6\n",
            "    GRADUATION YEAR       0.94      0.89      0.91        18\n",
            "           LOCATION       0.86      1.00      0.92        12\n",
            "               NAME       0.40      1.00      0.57        12\n",
            "                  O       0.00      0.00      0.00       118\n",
            "             SKILLS       0.92      0.93      0.92       144\n",
            "YEARS OF EXPERIENCE       0.85      0.92      0.88        12\n",
            "\n",
            "           accuracy                           0.66       467\n",
            "          macro avg       0.65      0.81      0.71       467\n",
            "       weighted avg       0.57      0.66      0.60       467\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}