{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7BGMs0IDQGr"
      },
      "source": [
        "Darbam ar PDF dokumentiem tiek ienistalēta speciāla bibliotēka."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV NLP Model"
      ],
      "metadata": {
        "id": "b79omcavb4Wn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY45zyVdC-EV",
        "outputId": "778bc218-1a94-477b-af95-52cd8cd753db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyacBm1QTuUz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from spacy.training import Example, offsets_to_biluo_tags\n",
        "from spacy.tokens import DocBin, span\n",
        "from spacy.util import minibatch, compounding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qGicWkNNg2Pg",
        "outputId": "81c9a922-5a63-4656-cd05-10b0dfadcaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/daviszars/nlp_cv_parser.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ObO_9eAG1p-",
        "outputId": "5d6d8fcf-be13-4505-d889-aeb9144cc368"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp_cv_parser'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 27 (delta 3), reused 27 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (27/27), 7.35 MiB | 18.17 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DesCnPHPSQ07"
      },
      "source": [
        "NER model training / preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gPNqV802Ixi"
      },
      "outputs": [],
      "source": [
        "def align_entities_with_tokens(nlp, text, entities):\n",
        "    doc = nlp.make_doc(text)\n",
        "    valid_entities = []\n",
        "    for start, end, label in entities:\n",
        "        span = doc.char_span(start, end, alignment_mode=\"contract\")\n",
        "        if span is not None:\n",
        "            valid_entities.append((span.start_char, span.end_char, label))\n",
        "    return valid_entities\n",
        "\n",
        "def preprocess_training_data(train_data, nlp):\n",
        "    processed_data = []\n",
        "    for text, annotations in train_data:\n",
        "        entities = annotations['entities']\n",
        "        aligned_entities = align_entities_with_tokens(nlp, text, entities)\n",
        "        if aligned_entities:\n",
        "            processed_data.append((text, {'entities': aligned_entities}))\n",
        "        else:\n",
        "            print(f\"Skipping misaligned entity: {entities} in text: {text[:50]}\")\n",
        "    return processed_data\n",
        "\n",
        "def train_model(train_data, iterations=10):\n",
        "    nlp = spacy.blank(\"en\")  # blank English model\n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.add_pipe('ner', last=True)\n",
        "\n",
        "    # we add labels to the NER pipeline\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations['entities']:\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "\n",
        "    train_data = preprocess_training_data(train_data, nlp)\n",
        "\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train the NER pipeline\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(iterations):\n",
        "            print(f\"Iteration {itn + 1}/{iterations}\")\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
        "                try:\n",
        "                    nlp.update(examples, drop=0.5, sgd=optimizer, losses=losses)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error updating with batch: {batch}\")\n",
        "                    print(e)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    return nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDp17y_LSuYq"
      },
      "source": [
        "Testing / Validating misalignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YXKodg4C8nrg"
      },
      "outputs": [],
      "source": [
        "'''file_path = \"nlp_cv_parser/train_data.json\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    aligned_train_data = json.load(file)\n",
        "\n",
        "# Inspect the first few entries\n",
        "for i, entry in enumerate(aligned_train_data[:5]):  # Display first 5 entries\n",
        "    text, annotations = entry\n",
        "    print(f\"Entry {i + 1}:\")\n",
        "    print(f\"Text: {text[:100]}...\")  # Display only first 100 characters\n",
        "    print(f\"Annotations: {annotations}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def debug_alignment(data):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    misaligned_entries = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        biluo_tags = offsets_to_biluo_tags(doc, entities)\n",
        "\n",
        "        if '-' in biluo_tags:\n",
        "            misaligned_entries.append({\n",
        "                'text': text,\n",
        "                'entities': entities,\n",
        "                'tags': biluo_tags\n",
        "            })\n",
        "\n",
        "    return misaligned_entries\n",
        "\n",
        "misaligned_entries = debug_alignment(aligned_train_data)\n",
        "for entry in misaligned_entries\n",
        "    print(f\"Text: {entry['text']}\")\n",
        "    print(f\"Entities: {entry['entities']}\")\n",
        "    print(f\"BILUO Tags: {entry['tags']}\") # \"-\" is misaligned\n",
        "    print(\"\\n\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wzxIgh2Syz5"
      },
      "source": [
        "Training / saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5ybxAs3DT6-T",
        "outputId": "2df5f052-8b80-4eb3-b1ab-5e732bc2b164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/100\n",
            "Losses {'ner': 21999.84887876154}\n",
            "Iteration 2/100\n",
            "Losses {'ner': 3730.1003680919875}\n",
            "Iteration 3/100\n",
            "Losses {'ner': 3546.363207352472}\n",
            "Iteration 4/100\n",
            "Losses {'ner': 3647.454556387455}\n",
            "Iteration 5/100\n",
            "Losses {'ner': 3504.937798037518}\n",
            "Iteration 6/100\n",
            "Losses {'ner': 3696.9023374014246}\n",
            "Iteration 7/100\n",
            "Losses {'ner': 2945.993383268496}\n",
            "Iteration 8/100\n",
            "Losses {'ner': 2792.722916814341}\n",
            "Iteration 9/100\n",
            "Losses {'ner': 2907.0753568093514}\n",
            "Iteration 10/100\n",
            "Losses {'ner': 2563.8826700847103}\n",
            "Iteration 11/100\n",
            "Losses {'ner': 2723.5998962630447}\n",
            "Iteration 12/100\n",
            "Losses {'ner': 3042.909618486358}\n",
            "Iteration 13/100\n",
            "Losses {'ner': 2560.241305627979}\n",
            "Iteration 14/100\n",
            "Losses {'ner': 2594.31771974024}\n",
            "Iteration 15/100\n",
            "Losses {'ner': 2526.9499360683217}\n",
            "Iteration 16/100\n",
            "Losses {'ner': 2445.042784533822}\n",
            "Iteration 17/100\n",
            "Losses {'ner': 2294.8190113108258}\n",
            "Iteration 18/100\n",
            "Losses {'ner': 2308.2591326358606}\n",
            "Iteration 19/100\n",
            "Losses {'ner': 2401.826136999996}\n",
            "Iteration 20/100\n",
            "Losses {'ner': 2134.6318473807623}\n",
            "Iteration 21/100\n",
            "Losses {'ner': 2248.1215234360466}\n",
            "Iteration 22/100\n",
            "Losses {'ner': 2093.867791343917}\n",
            "Iteration 23/100\n",
            "Losses {'ner': 2129.6525471518075}\n",
            "Iteration 24/100\n",
            "Losses {'ner': 2240.6231101082108}\n",
            "Iteration 25/100\n",
            "Losses {'ner': 2151.107288554977}\n",
            "Iteration 26/100\n",
            "Losses {'ner': 2066.136766685125}\n",
            "Iteration 27/100\n",
            "Losses {'ner': 2217.8805292527722}\n",
            "Iteration 28/100\n",
            "Losses {'ner': 2115.8948983657287}\n",
            "Iteration 29/100\n",
            "Losses {'ner': 2061.5125652598235}\n",
            "Iteration 30/100\n",
            "Losses {'ner': 2403.247154840782}\n",
            "Iteration 31/100\n",
            "Losses {'ner': 2057.4211183810576}\n",
            "Iteration 32/100\n",
            "Losses {'ner': 1959.1406434118956}\n",
            "Iteration 33/100\n",
            "Losses {'ner': 2033.3650078321657}\n",
            "Iteration 34/100\n",
            "Losses {'ner': 1935.6853213602556}\n",
            "Iteration 35/100\n",
            "Losses {'ner': 1793.2520822315814}\n",
            "Iteration 36/100\n",
            "Losses {'ner': 1906.1780320606815}\n",
            "Iteration 37/100\n",
            "Losses {'ner': 1940.6736503014984}\n",
            "Iteration 38/100\n",
            "Losses {'ner': 1825.9130935172143}\n",
            "Iteration 39/100\n",
            "Losses {'ner': 1749.9568227606007}\n",
            "Iteration 40/100\n",
            "Losses {'ner': 1896.1827711353774}\n",
            "Iteration 41/100\n",
            "Losses {'ner': 1733.5175500742425}\n",
            "Iteration 42/100\n",
            "Losses {'ner': 1761.5547260079263}\n",
            "Iteration 43/100\n",
            "Losses {'ner': 1672.3987193008088}\n",
            "Iteration 44/100\n",
            "Losses {'ner': 1672.3857891190464}\n",
            "Iteration 45/100\n",
            "Losses {'ner': 1874.0354963196014}\n",
            "Iteration 46/100\n",
            "Losses {'ner': 1698.7983573010176}\n",
            "Iteration 47/100\n",
            "Losses {'ner': 1647.3056071819306}\n",
            "Iteration 48/100\n",
            "Losses {'ner': 1613.5609554616522}\n",
            "Iteration 49/100\n",
            "Losses {'ner': 1563.2218278980295}\n",
            "Iteration 50/100\n",
            "Losses {'ner': 1568.753947250706}\n",
            "Iteration 51/100\n",
            "Losses {'ner': 1611.5560364159983}\n",
            "Iteration 52/100\n",
            "Losses {'ner': 1535.1698141480024}\n",
            "Iteration 53/100\n",
            "Losses {'ner': 1623.7812333038582}\n",
            "Iteration 54/100\n",
            "Losses {'ner': 1552.5095207929196}\n",
            "Iteration 55/100\n",
            "Losses {'ner': 1494.0454553081831}\n",
            "Iteration 56/100\n",
            "Losses {'ner': 1470.7191021192557}\n",
            "Iteration 57/100\n",
            "Losses {'ner': 1517.6962537139195}\n",
            "Iteration 58/100\n",
            "Losses {'ner': 1505.45357495284}\n",
            "Iteration 59/100\n",
            "Losses {'ner': 1522.5968518394395}\n",
            "Iteration 60/100\n",
            "Losses {'ner': 1529.7730517340208}\n",
            "Iteration 61/100\n",
            "Losses {'ner': 1427.5507504118032}\n",
            "Iteration 62/100\n",
            "Losses {'ner': 1511.9077247860291}\n",
            "Iteration 63/100\n",
            "Losses {'ner': 1547.0989956940275}\n",
            "Iteration 64/100\n",
            "Losses {'ner': 1486.373654524394}\n",
            "Iteration 65/100\n",
            "Losses {'ner': 1410.50761807435}\n",
            "Iteration 66/100\n",
            "Losses {'ner': 1403.287990260446}\n",
            "Iteration 67/100\n",
            "Losses {'ner': 1382.1797510719684}\n",
            "Iteration 68/100\n",
            "Losses {'ner': 1390.4235609820869}\n",
            "Iteration 69/100\n",
            "Losses {'ner': 1389.1368031149786}\n",
            "Iteration 70/100\n",
            "Losses {'ner': 1366.8870636957113}\n",
            "Iteration 71/100\n",
            "Losses {'ner': 1372.802028385509}\n",
            "Iteration 72/100\n",
            "Losses {'ner': 1366.5515809966178}\n",
            "Iteration 73/100\n",
            "Losses {'ner': 1317.9577739423607}\n",
            "Iteration 74/100\n",
            "Losses {'ner': 1274.9653339545605}\n",
            "Iteration 75/100\n",
            "Losses {'ner': 1334.4886261127938}\n",
            "Iteration 76/100\n",
            "Losses {'ner': 1242.435356725037}\n",
            "Iteration 77/100\n",
            "Losses {'ner': 1321.6094595025118}\n",
            "Iteration 78/100\n",
            "Losses {'ner': 1308.306992717327}\n",
            "Iteration 79/100\n",
            "Losses {'ner': 1327.7916520017916}\n",
            "Iteration 80/100\n",
            "Losses {'ner': 1263.5730011748785}\n",
            "Iteration 81/100\n",
            "Losses {'ner': 1260.1947599451628}\n",
            "Iteration 82/100\n",
            "Losses {'ner': 1294.3328749096372}\n",
            "Iteration 83/100\n",
            "Losses {'ner': 1258.004698489499}\n",
            "Iteration 84/100\n",
            "Losses {'ner': 1282.0355634050038}\n",
            "Iteration 85/100\n",
            "Losses {'ner': 1260.2766742486021}\n",
            "Iteration 86/100\n",
            "Losses {'ner': 1257.5107438645343}\n",
            "Iteration 87/100\n",
            "Losses {'ner': 1199.5958466107095}\n",
            "Iteration 88/100\n",
            "Losses {'ner': 1281.3921150651815}\n",
            "Iteration 89/100\n",
            "Losses {'ner': 1278.7480448742165}\n",
            "Iteration 90/100\n",
            "Losses {'ner': 1234.1784645237044}\n",
            "Iteration 91/100\n",
            "Losses {'ner': 1197.0524153537917}\n",
            "Iteration 92/100\n",
            "Losses {'ner': 1244.6067672650224}\n",
            "Iteration 93/100\n",
            "Losses {'ner': 1160.7990901634828}\n",
            "Iteration 94/100\n",
            "Losses {'ner': 1116.297999593345}\n",
            "Iteration 95/100\n",
            "Losses {'ner': 1093.7245432711802}\n",
            "Iteration 96/100\n",
            "Losses {'ner': 1133.2718837164139}\n",
            "Iteration 97/100\n",
            "Losses {'ner': 1174.0308611636208}\n",
            "Iteration 98/100\n",
            "Losses {'ner': 1221.0847907758969}\n",
            "Iteration 99/100\n",
            "Losses {'ner': 1150.0359263856226}\n",
            "Iteration 100/100\n",
            "Losses {'ner': 1100.1991820657825}\n"
          ]
        }
      ],
      "source": [
        "# Load the cleaned data\n",
        "file_path = \"nlp_cv_parser/train_data.json\" # pagaidām manuāli jāieliek colab, uz beigām varēšu github ielikt, lai būtu download links\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Train the model\n",
        "nlp = train_model(train_data, iterations=100)\n",
        "\n",
        "# Save the model\n",
        "nlp.to_disk(\"nlp_cv_parser/ner_model_spacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing NER model on test input data"
      ],
      "metadata": {
        "id": "sYxuDoCFugCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kty5XmubSUtp",
        "outputId": "4734b4e7-6350-408e-c2d0-d77d10fdd648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities in the test input:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_cv_spacy\")\n",
        "\n",
        "'''\n",
        "test_input = \"\"\"\n",
        "Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\n",
        "\"\"\"\n",
        "'''\n",
        "# test_input = \"\"\"\n",
        "# Alice Clark  AI / Machine Learning    Delhi, India Email me on Indeed  •  20+ years of experience in data handling, design, and development  •  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to  data warehousing and business intelligence  •  Database: Experience in database designing, scalability, back-up and recovery, writing and  optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.  Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure,  Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake  analytics(U-SQL)  Willing to relocate anywhere    WORK EXPERIENCE  Software Engineer  Microsoft – Bangalore, Karnataka  January 2000 to Present  1. Microsoft Rewards Live dashboards:  Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping  online. Microsoft Rewards members can earn points when searching with Bing, browsing with  Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft  Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft  rewards website. Rewards live dashboards gives a live picture of usage world-wide and by  markets like US, Canada, Australia, new user registration count, top/bottom performing rewards  offers, orders stats and weekly trends of user activities, orders and new user registrations. the  PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.  Technology/Tools used    EDUCATION  Indian Institute of Technology – Mumbai  2001    SKILLS  Machine Learning, Natural Language Processing, and Big Data Handling    ADDITIONAL INFORMATION  Professional Skills  • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal  skills with ability to interact with individuals at all the levels  • Quick learner and maintains cordial relationship with project manager and team members and  good performer both in team and independent job environments  • Positive attitude towards superiors &amp; peers  • Supervised junior developers throughout project lifecycle and provided technical assistance\n",
        "# \"\"\"\n",
        "test_input = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Process the input string using the model\n",
        "doc = nlp(test_input)\n",
        "\n",
        "print(\"Entities in the test input:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.start_char}, {ent.end_char}): {ent.label_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTweo4IgTJJf"
      },
      "source": [
        "Convert data from pdf's"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"./trained_nlp_41min\"\n",
        "nlp.to_disk(model_name)"
      ],
      "metadata": {
        "id": "qTVLeAfYTQnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract the trained model"
      ],
      "metadata": {
        "id": "QAorJuaXT4xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/trained_nlp_41min"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoeJihrETrx5",
        "outputId": "1dacc549-9232-46be-9b0e-134edf08e294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/trained_nlp_41min/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/config.cfg (deflated 59%)\n",
            "  adding: content/trained_nlp_41min/ner/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/ner/model (deflated 8%)\n",
            "  adding: content/trained_nlp_41min/ner/moves (deflated 75%)\n",
            "  adding: content/trained_nlp_41min/ner/cfg (deflated 33%)\n",
            "  adding: content/trained_nlp_41min/vocab/ (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/key2row (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/strings.json (deflated 75%)\n",
            "  adding: content/trained_nlp_41min/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/trained_nlp_41min/vocab/vectors (deflated 45%)\n",
            "  adding: content/trained_nlp_41min/meta.json (deflated 50%)\n",
            "  adding: content/trained_nlp_41min/tokenizer (deflated 81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhPzkgCaJ0uW"
      },
      "source": [
        "Usable for both, create more training data and for end testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwwTM13oTIuP",
        "outputId": "94c18f06-8b08-4a39-c3fa-7f227baa039a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kristen Connelly\n",
            "V I D E O  P R O D U C T A S S I S TA N T\n",
            "Proﬁle\n",
            "Three-year online Video Production Assistant for commercials and short films. \n",
            "Competent with Premiere Pro for trimming short segments to required lengths. 20% more \n",
            "likes and comments due to re-sequenced scenes to enhance audience satisfaction.\n",
            "Employment History\n",
            "Video Production Assistant, Blue Penguin Designs, Bar Bigha\n",
            "F E B R U A RY 2 0 2 1  —  P R E S E N T\n",
            "Responsible for supporting a large scale production team of 100 people, \n",
            "throughout production, including development, pre-production, post-production, \n",
            "principal photography, and distribution.\n",
            "• Managed the bookings for guests, the green room traffic, and any guest suites.\n",
            "• Researched story ideas and then compiled footnotes and gathered footage for \n",
            "presentation.\n",
            "• Completed Video Production-scriptwriting, audio editing, and mixing using \n",
            "the Avid non-linear editing system.\n",
            "• Maintained and assisted with Production Acquisition.\n",
            "Video Production Assistant, Botle Bob Advertising, Opelousas\n",
            "J A N U A RY 2 0 1 9  —  F E B R U A RY 2 0 2 1\n",
            "Responsible for organizing and preparing visual and audio equipment to be used on video \n",
            "projects for clients such as Pfizer, Knoll Pharmaceuticals, and Merck.\n",
            "• Helped edit graphics and video projects from conception to creation.\n",
            "• Collaborated with the production team to provide content, digital assets, \n",
            "presentations, and other video needs.\n",
            "• Built, edited, and polished videography proposals for business clients.\n",
            "• Staged and set up audio and video equipment for studio and on-location \n",
            "shoots.\n",
            "• Performed minor maintenance on video and audio equipment and managed \n",
            "any required vendor repair.\n",
            "Education\n",
            "BA in Film and Television, Boston University, Boston\n",
            "F E B R U A RY 2 0 2 1  —  P R E S E N T\n",
            "• Related Coursework: Storytelling for Film and Television, American Masterworks, \n",
            "Television Drama, The Holocaust, Film Industry, International Film.\n",
            "Advanced Course in Digital Video Editing, ADMEC Multimedia Institute, \n",
            "Online\n",
            "J A N U A RY 2 0 1 8  —  J U LY 2 0 1 8\n",
            "2015 – Advanced Course in Digital Video Editing, ADMEC Multimedia Institute, \n",
            "Online\n",
            "Courses\n",
            "Hootsuite Certified Professional, Hootsuite Media, Albany, NY\n",
            "J A N U A RY 2 0 2 0  —  F E B R U A RY 2 0 2 1\n",
            "Adobe CS5 Certified,, University of Delaware, Newark, DE.\n",
            "J A N U A RY 2 0 2 0  —  D E C E M B E R  2 0 2 0\n",
            "Details\n",
            "1515 Pacific Ave\n",
            "Los Angeles, CA 90291\n",
            "United States\n",
            "3868683442\n",
            "email@email.com\n",
            "D R I V I N G  L I C E N S E\n",
            "Full\n",
            "P L AC E  O F B I RT H\n",
            "San Antonio\n",
            "Links\n",
            "LinkedIn\n",
            "Resume Templates\n",
            "Build this template\n",
            "Pinterest\n",
            "Skills\n",
            "Call Sheets & Sides\n",
            "Adobe Premiere Pro\n",
            "Camera Boom, Light Boom, Mic \n",
            "Boom\n",
            "DaVinci Resolve\n",
            "Languages\n",
            "English\n",
            "Dutch; Flemish\n",
            "Hobbies\n",
            "Mountainbiking, Track Athletics, \n",
            "Choir\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys, fitz\n",
        "fname = 'nlp_cv_parser/Berlin-Simple-Resume-Template.pdf'\n",
        "doc = fitz.open(fname)\n",
        "text = \"\"\n",
        "for page in range(doc.page_count):\n",
        "    page_content = doc.load_page(page)  # Load each page\n",
        "    page_text = page_content.get_text(\"text\")  # Extract text from the page\n",
        "    text += page_text  # Append the extracted text to the all_text string\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n-0oAAsCMEBx",
        "outputId": "738ea00e-9f8d-4a0a-ed40-cdb686c6e571",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nlp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9e197e831ea5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# List all tokens in the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ],
      "source": [
        "vocab = nlp.vocab\n",
        "\n",
        "# List all tokens in the vocabulary\n",
        "tokens = [word.text for word in vocab if word.is_alpha]\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pTX_7aHMqU9",
        "outputId": "824fb45f-4341-4de5-9e68-c9e6c11c71bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NER Labels:\n",
            "('College Name', 'Companies worked at', 'Degree', 'Designation', 'Email Address', 'Graduation Year', 'Location', 'Name', 'Skills', 'UNKNOWN', 'Years of Experience')\n"
          ]
        }
      ],
      "source": [
        "# Get the entity labels\n",
        "ner_labels = nlp.get_pipe(\"ner\").labels\n",
        "\n",
        "print(\"NER Labels:\")\n",
        "print(ner_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW-hKLoSJYRH",
        "outputId": "5a9e2c22-5563-42a7-9f57-edfa802fef4d",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jason 0-4\n",
            "Miller 6-11\n",
            "Amazon 13-18\n",
            "Associate 20-28\n",
            "Profile 30-36\n",
            "Experienced 38-48\n",
            "Amazon 50-55\n",
            "Associate 57-65\n",
            "with 67-70\n",
            "five 72-75\n",
            "years’ 77-82\n",
            "tenure 84-89\n",
            "in 91-92\n",
            "a 94-94\n",
            "shipping 96-103\n",
            "yard 105-108\n",
            "setting, 111-118\n",
            "maintaining 120-130\n",
            "an 132-133\n",
            "average 135-141\n",
            "picking/packing 143-157\n",
            "speed 159-163\n",
            "of 165-166\n",
            "98%. 168-171\n",
            "Holds 173-177\n",
            "a 179-179\n",
            "zero 182-185\n",
            "error% 187-192\n",
            "score 194-198\n",
            "in 200-201\n",
            "adhering 203-210\n",
            "to 212-213\n",
            "packing 215-221\n",
            "specs 223-227\n",
            "and 229-231\n",
            "97% 233-235\n",
            "error-free 237-246\n",
            "ratio 248-252\n",
            "on 255-256\n",
            "packing 258-264\n",
            "records. 266-273\n",
            "Completed 275-283\n",
            "a 285-285\n",
            "certificate 287-297\n",
            "in 299-300\n",
            "Warehouse 302-310\n",
            "Sanitation 312-321\n",
            "and 323-325\n",
            "has 328-330\n",
            "a 332-332\n",
            "valid 334-338\n",
            "commercial 340-349\n",
            "driver’s 351-358\n",
            "license. 360-367\n",
            "Employment 369-378\n",
            "History 380-386\n",
            "Amazon 388-393\n",
            "Warehouse 395-403\n",
            "Associate 405-413\n",
            "at 415-416\n",
            "Amazon, 418-424\n",
            "Miami 426-430\n",
            "Gardens 432-438\n",
            "January 440-446\n",
            "2021 448-451\n",
            "— 453-453\n",
            "July 455-458\n",
            "2022 460-463\n",
            "Performed 465-473\n",
            "all 475-477\n",
            "warehouse 479-487\n",
            "laborer 489-495\n",
            "duties 497-502\n",
            "such 504-507\n",
            "as 509-510\n",
            "packing, 512-519\n",
            "picking, 521-528\n",
            "counting, 531-539\n",
            "record 541-546\n",
            "keeping, 548-555\n",
            "and 557-559\n",
            "maintaining 561-571\n",
            "a 573-573\n",
            "clean 575-579\n",
            "area. 581-585\n",
            "• 587-587\n",
            "Consistently 589-600\n",
            "maintained 602-611\n",
            "picking/packing 613-627\n",
            "speeds 629-634\n",
            "in 636-637\n",
            "the 639-641\n",
            "98th 643-646\n",
            "percentile. 649-659\n",
            "• 661-661\n",
            "Picked 663-668\n",
            "all 670-672\n",
            "orders 674-679\n",
            "with 681-684\n",
            "100% 686-689\n",
            "accuracy 691-698\n",
            "despite 700-706\n",
            "high 708-711\n",
            "speeds. 713-719\n",
            "• 721-721\n",
            "Maintained 723-732\n",
            "a 734-734\n",
            "clean 736-740\n",
            "work 742-745\n",
            "area, 747-751\n",
            "meeting 753-759\n",
            "97.5% 761-765\n",
            "of 767-768\n",
            "the 770-772\n",
            "inspection 774-783\n",
            "requirements. 786-798\n",
            "Laboratory 800-809\n",
            "Inventory 811-819\n",
            "Assistant 821-829\n",
            "at 832-833\n",
            "Dunrea 836-841\n",
            "Laboratories, 843-855\n",
            "Orlando 857-863\n",
            "January 865-871\n",
            "2019 873-876\n",
            "— 878-878\n",
            "December 880-887\n",
            "2020 889-892\n",
            "Full-time 894-902\n",
            "lab 904-906\n",
            "assistant 908-916\n",
            "in 918-919\n",
            "a 921-921\n",
            "small, 923-928\n",
            "regional 930-937\n",
            "laboratory 939-948\n",
            "tasked 950-955\n",
            "with 957-960\n",
            "· 962-962\n",
            "participating 965-977\n",
            "in 979-980\n",
            "Kaizen 982-987\n",
            "Events, 989-995\n",
            "Gemba 997-1001\n",
            "walks, 1003-1008\n",
            "and 1010-1012\n",
            "5S 1014-1015\n",
            "to 1017-1018\n",
            "remove 1020-1025\n",
            "barriers 1027-1034\n",
            "and 1037-1039\n",
            "improve 1041-1047\n",
            "productivity. 1049-1061\n",
            "• 1063-1063\n",
            "Filled 1065-1070\n",
            "the 1072-1074\n",
            "warehouse 1076-1084\n",
            "helper 1086-1091\n",
            "job 1093-1095\n",
            "description, 1097-1108\n",
            "which 1110-1114\n",
            "involved 1116-1123\n",
            "picking, 1126-1133\n",
            "packing, 1135-1142\n",
            "shipping, 1144-1152\n",
            "inventory 1154-1162\n",
            "management, 1164-1174\n",
            "and 1176-1178\n",
            "cleaning 1180-1187\n",
            "equipment. 1190-1199\n",
            "• 1201-1201\n",
            "Saved 1203-1207\n",
            "12% 1209-1211\n",
            "on 1213-1214\n",
            "UPS 1216-1218\n",
            "orders 1220-1225\n",
            "by 1227-1228\n",
            "staying 1230-1236\n",
            "on 1238-1239\n",
            "top 1241-1243\n",
            "of 1245-1246\n",
            "special 1248-1254\n",
            "deals. 1256-1261\n",
            "• 1263-1263\n",
            "Cut 1265-1267\n",
            "down 1269-1272\n",
            "storage 1274-1280\n",
            "waste 1282-1286\n",
            "by 1288-1289\n",
            "23% 1291-1293\n",
            "by 1295-1296\n",
            "switching 1298-1306\n",
            "to 1308-1309\n",
            "a 1311-1311\n",
            "Kanban 1313-1318\n",
            "system. 1320-1326\n",
            "Education 1328-1336\n",
            "Associates 1338-1347\n",
            "Degree 1349-1354\n",
            "in 1356-1357\n",
            "Logistics 1359-1367\n",
            "and 1369-1371\n",
            "Supply 1373-1378\n",
            "Chain 1380-1384\n",
            "Fundamentals, 1386-1398\n",
            "Atlanta 1401-1407\n",
            "Technical 1409-1417\n",
            "College, 1419-1426\n",
            "Atlanta 1428-1434\n",
            "January 1436-1442\n",
            "2021 1444-1447\n",
            "— 1449-1449\n",
            "July 1451-1454\n",
            "2022 1456-1459\n",
            "• 1461-1461\n",
            "Majors: 1463-1469\n",
            "Warehousing 1471-1481\n",
            "Operations, 1483-1493\n",
            "Logistics 1495-1503\n",
            "and 1505-1507\n",
            "Distribution 1509-1520\n",
            "Practices 1523-1531\n",
            "• 1533-1533\n",
            "Minors: 1535-1541\n",
            "Inventory 1543-1551\n",
            "Systems, 1553-1560\n",
            "Supply 1562-1567\n",
            "Chain 1569-1573\n",
            "Principles 1575-1584\n",
            "Courses 1586-1592\n",
            "Online 1594-1599\n",
            "Graduate 1601-1608\n",
            "Certificate 1610-1620\n",
            "in 1622-1623\n",
            "Warehousing 1625-1635\n",
            "& 1637-1637\n",
            "Supply 1639-1644\n",
            "Chain 1646-1650\n",
            "Management, 1653-1663\n",
            "Southern 1665-1672\n",
            "New 1674-1676\n",
            "Hampshire 1678-1686\n",
            "University 1688-1697\n",
            "(SNHU), 1699-1705\n",
            "NH. 1707-1709\n",
            "July 1711-1714\n",
            "2022 1716-1719\n",
            "— 1721-1721\n",
            "July 1723-1726\n",
            "2022 1728-1731\n",
            "Details 1733-1739\n",
            "1515 1741-1744\n",
            "Pacific 1746-1752\n",
            "Ave 1754-1756\n",
            "Los 1758-1760\n",
            "Angeles, 1762-1769\n",
            "CA 1771-1772\n",
            "90291 1774-1778\n",
            "United 1780-1785\n",
            "States 1787-1792\n",
            "3868683442 1794-1803\n",
            "email@email.com 1805-1819\n",
            "Place 1821-1825\n",
            "of 1827-1828\n",
            "birth 1830-1834\n",
            "San 1836-1838\n",
            "Antonio 1840-1846\n",
            "Driving 1848-1854\n",
            "license 1856-1862\n",
            "Full 1864-1867\n",
            "Links 1869-1873\n",
            "LinkedIn 1875-1882\n",
            "Pinterest 1884-1892\n",
            "Resume 1894-1899\n",
            "Templates 1901-1909\n",
            "Build 1911-1915\n",
            "this 1917-1920\n",
            "template 1922-1929\n",
            "Skills 1931-1936\n",
            "Cleaning 1938-1945\n",
            "Equipment 1947-1955\n",
            "Cleaning 1957-1964\n",
            "Equipment 1966-1974\n",
            "Mathematics 1976-1986\n",
            "Cleaning 1988-1995\n",
            "Equipment 1997-2005\n",
            "Deep 2007-2010\n",
            "Sanitation 2012-2021\n",
            "Practices 2023-2031\n",
            "Hobbies 2033-2039\n",
            "Action 2041-2046\n",
            "Cricket, 2048-2055\n",
            "Rugby, 2057-2062\n",
            "Athletics 2064-2072\n",
            "Languages 2074-2082\n",
            "English 2084-2090\n",
            "Spanish 2092-2098\n",
            "Warehousing, 2100-2111\n",
            "Operations, 2113-2123\n",
            "and 2125-2127\n",
            "Disposal 2129-2136\n",
            "Course, 2138-2144\n",
            "Graduate 2146-2153\n",
            "School 2155-2160\n",
            "USA, 2163-2166\n",
            "Washington 2168-2177\n",
            "DC. 2179-2181\n",
            "January 2183-2189\n",
            "2021 2191-2194\n",
            "— 2196-2196\n",
            "May 2198-2200\n",
            "2021 2202-2205\n",
            "Achievements 2207-2218\n",
            "• 2220-2220\n",
            "Decrease 2222-2229\n",
            "the 2231-2233\n",
            "errors 2235-2240\n",
            "rate 2242-2245\n",
            "and 2247-2249\n",
            "QC 2251-2252\n",
            "several 2254-2260\n",
            "more 2262-2265\n",
            "orders 2267-2272\n",
            "that 2274-2277\n",
            "we 2279-2280\n",
            "ship 2282-2285\n",
            "per 2288-2290\n",
            "day. 2292-2295\n",
            "• 2297-2297\n",
            "Awarded 2299-2305\n",
            "\"Employee 2307-2315\n",
            "of 2317-2318\n",
            "the 2320-2322\n",
            "month\" 2324-2329\n",
            "due 2331-2333\n",
            "to 2335-2336\n",
            "consistent 2338-2347\n",
            "attendance, 2349-2359\n",
            "punctuality, 2362-2373\n",
            "and 2375-2377\n",
            "performance. 2379-2390\n",
            "• 2392-2392\n",
            "Managed 2394-2400\n",
            "workflow 2402-2409\n",
            "of 2411-2412\n",
            "associates 2414-2423\n",
            "for 2425-2427\n",
            "the 2429-2431\n",
            "FC. 2433-2435\n"
          ]
        }
      ],
      "source": [
        "def print_word_locations(text):\n",
        "    words = text.split()\n",
        "    start_pos = 0\n",
        "\n",
        "    for word in words:\n",
        "        start_pos = text.find(word, start_pos)\n",
        "        end_pos = start_pos + len(word) - 1\n",
        "        print(f\"{word} {start_pos}-{end_pos}\")\n",
        "        start_pos += len(word)\n",
        "\n",
        "print_word_locations(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert sample data set to uppercase so the Ner-Anotator can be used."
      ],
      "metadata": {
        "id": "tUi9oezWGEHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra samples taken from: https://www.resumeviking.com/templates/\n",
        "Tagged with: https://tecoholic.github.io/ner-annotator/"
      ],
      "metadata": {
        "id": "Qb7wCxGgLyXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def convert_tags_in_json(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Ensure the data is a list of dictionaries with 'tag' key\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"JSON file should contain a list of dictionaries\")\n",
        "\n",
        "        # Convert tags to uppercase\n",
        "        for item in data:\n",
        "            for tag in range(len(item[1]['entities'])):\n",
        "              item[1]['entities'][tag][2] = item[1]['entities'][tag][2].upper()\n",
        "\n",
        "        # Write the updated JSON back to the file\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(\"Tags converted to uppercase successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "file_path = 'nlp_cv_parser/train_data.json'\n",
        "convert_tags_in_json(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpXJP70rC5v9",
        "outputId": "5277143f-f8fe-48e2-9eeb-06d14fc0541b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tags converted to uppercase successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Job listing NLP"
      ],
      "metadata": {
        "id": "Nzdf4u4ydM80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"nlp_cv_parser/training_data_jobs.json\"\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Train the model\n",
        "nlp_job = train_model(train_data, iterations=100)\n",
        "\n",
        "# Save the model\n",
        "nlp_job.to_disk(\"nlp_cv_parser/ner_model_spacy\")"
      ],
      "metadata": {
        "id": "rVProyYndPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Ensure the data is a list of dictionaries with entities key\n",
        "if not isinstance(data, list):\n",
        "    raise ValueError(\"JSON file should contain a list of dictionaries\")\n",
        "\n",
        "for item in data:\n",
        "    for tag in range(len(item[1]['entities'])):\n",
        "      item[1]['entities'][tag][2] = item[1]['entities'][tag][2].upper()\n",
        "\n",
        "# Write the updated json back to the file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "WRz4WWOrhxQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jobs NER model test"
      ],
      "metadata": {
        "id": "km_VMlXDSz82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_jobs_spacy\")\n",
        "\n",
        "with open(\"nlp_cv_parser/test_data_jobs.json\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "def create_examples(data, nlp):\n",
        "    examples = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        spans = [(start, end, label) for start, end, label in entities]\n",
        "        example = Example.from_dict(doc, {\"entities\": spans})\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "test_examples = create_examples(test_data, nlp)\n",
        "\n",
        "def evaluate_model(nlp, examples):\n",
        "    scorer = nlp.evaluate(examples)\n",
        "    return scorer\n",
        "\n",
        "scorer = evaluate_model(nlp, test_examples)\n",
        "\n",
        "print(f\"Precision: {scorer['ents_p']}\")\n",
        "print(f\"Recall: {scorer['ents_r']}\")\n",
        "print(f\"F1-score: {scorer['ents_f']}\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Extract true and predicted entities with their labels\n",
        "for example in test_examples:\n",
        "    gold_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in example.reference.ents]\n",
        "    pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in nlp(example.text).ents]\n",
        "\n",
        "    gold_map = { (start, end): label for start, end, label in gold_ents }\n",
        "\n",
        "    pred_map = { (start, end): label for start, end, label in pred_ents }\n",
        "\n",
        "    all_positions = set(gold_map.keys()).union(set(pred_map.keys()))\n",
        "\n",
        "    # Populate y_true and y_pred based on positions\n",
        "    for pos in all_positions:\n",
        "        y_true.append(gold_map.get(pos, 'O'))\n",
        "        y_pred.append(pred_map.get(pos, 'O'))\n",
        "\n",
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsj-uIPsq-cq",
        "outputId": "0abd3e6d-6717-40a0-b2ef-ba779a3a6f0f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3425925925925926\n",
            "Recall: 0.2813688212927757\n",
            "F1-score: 0.30897703549060546\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "      EXPERIENCE       0.37      0.39      0.38        36\n",
            "               O       0.00      0.00      0.00       124\n",
            "   QUALIFICATION       0.21      0.18      0.19        34\n",
            "    REQUIREMENTS       1.00      0.02      0.04        46\n",
            "RESPONSIBILITIES       0.42      0.38      0.40        97\n",
            "          SKILLS       0.27      0.32      0.29        50\n",
            "\n",
            "        accuracy                           0.19       387\n",
            "       macro avg       0.38      0.21      0.22       387\n",
            "    weighted avg       0.31      0.19      0.19       387\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV NER model test"
      ],
      "metadata": {
        "id": "1kLX_ogeSvM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"nlp_cv_parser/ner_model_cv_spacy\")\n",
        "\n",
        "with open(\"nlp_cv_parser/test_data.json\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "def create_examples(data, nlp):\n",
        "    examples = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        entities = annotations['entities']\n",
        "        spans = [(start, end, label) for start, end, label in entities]\n",
        "        example = Example.from_dict(doc, {\"entities\": spans})\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "test_examples = create_examples(test_data, nlp)\n",
        "\n",
        "def evaluate_model(nlp, examples):\n",
        "    scorer = nlp.evaluate(examples)\n",
        "    return scorer\n",
        "\n",
        "scorer = evaluate_model(nlp, test_examples)\n",
        "\n",
        "print(f\"Precision: {scorer['ents_p']}\")\n",
        "print(f\"Recall: {scorer['ents_r']}\")\n",
        "print(f\"F1-score: {scorer['ents_f']}\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Extract true and predicted entities with their labels\n",
        "for example in test_examples:\n",
        "    gold_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in example.reference.ents]\n",
        "    pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in nlp(example.text).ents]\n",
        "\n",
        "    gold_map = { (start, end): label for start, end, label in gold_ents }\n",
        "\n",
        "    pred_map = { (start, end): label for start, end, label in pred_ents }\n",
        "\n",
        "    all_positions = set(gold_map.keys()).union(set(pred_map.keys()))\n",
        "\n",
        "    # Populate y_true and y_pred based on positions\n",
        "    for pos in all_positions:\n",
        "        y_true.append(gold_map.get(pos, 'O'))\n",
        "        y_pred.append(pred_map.get(pos, 'O'))\n",
        "\n",
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOFg3DS5SLB1",
        "outputId": "73741a47-b4b1-4fca-ca5e-ddaffe5010cc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Abhishek Jha Application Development Associate - A...\" with entities \"[(0, 11, 'NAME'), (13, 45, 'DESIGNATION'), (49, 57...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Afreen Jamadar Active member of IIIT Committee in ...\" with entities \"[(0, 13, 'NAME'), (62, 67, 'LOCATION'), (104, 147,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Akhil Yadav Polemaina Hyderabad, Telangana - Email...\" with entities \"[(0, 20, 'NAME'), (22, 30, 'LOCATION'), (65, 116, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Alok Khandai Operational Analyst (SQL DBA) Enginee...\" with entities \"[(0, 11, 'NAME'), (54, 59, 'COMPANIES WORKED AT'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ananya Chavan lecturer - oracle tutorials  Mumbai,...\" with entities \"[(0, 12, 'NAME'), (14, 21, 'DESIGNATION'), (25, 40...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Anvitha Rao Automation developer  - Email me on In...\" with entities \"[(0, 10, 'NAME'), (12, 31, 'DESIGNATION'), (56, 96...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"arjun ks Senior Program coordinator - oracle India...\" with entities \"[(0, 7, 'NAME'), (9, 34, 'DESIGNATION'), (38, 43, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Arun Elumalai QA Tester  Chennai, Tamil Nadu - Ema...\" with entities \"[(0, 12, 'NAME'), (14, 23, 'DESIGNATION'), (25, 31...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ashalata Bisoyi Transaction Processor - Oracle Ind...\" with entities \"[(955, 990, 'DEGREE'), (993, 1021, 'COLLEGE NAME')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ashok Kunam Team Lead - Microsoft  - Email me on I...\" with entities \"[(0, 10, 'NAME'), (12, 20, 'DESIGNATION'), (24, 32...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Asish Ratha Subject matter Expert - Accenture  Che...\" with entities \"[(0, 10, 'NAME'), (12, 32, 'DESIGNATION'), (36, 44...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Avin Sharma Senior Associate Consultant - Infosys ...\" with entities \"[(0, 10, 'NAME'), (12, 39, 'DESIGNATION'), (42, 56...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ayesha B Team member - Oracle  Bangalore, Karnatak...\" with entities \"[(0, 6, 'NAME'), (9, 19, 'DESIGNATION'), (23, 28, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ayushi Srivastava Senior Analyst - Cisco  New Delh...\" with entities \"[(0, 16, 'NAME'), (42, 50, 'LOCATION'), (81, 127, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Bhawana Daf Pune, Maharashtra - Email me on Indeed...\" with entities \"[(0, 10, 'NAME'), (12, 15, 'LOCATION'), (52, 93, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Darshan G. Financial Analyst - Oracle  Bengaluru, ...\" with entities \"[(0, 9, 'NAME'), (11, 27, 'DESIGNATION'), (31, 36,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dhanushkodi Raj Technology Analyst - Infosys Limit...\" with entities \"[(0, 14, 'NAME'), (16, 33, 'DESIGNATION'), (37, 52...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dinesh Reddy Deployed chef for configuration manag...\" with entities \"[(0, 11, 'NAME'), (13, 69, 'DESIGNATION'), (73, 77...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dipesh Gulati Co-coordinator of CODE HUNTER at CGC...\" with entities \"[(0, 12, 'NAME'), (57, 61, 'LOCATION'), (64, 68, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dushyant Bhatt BI / Big Data/ Azure  Hyderabad-Dec...\" with entities \"[(0, 13, 'NAME'), (37, 45, 'LOCATION'), (729, 738,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Govardhana K Senior Software Engineer  Bengaluru, ...\" with entities \"[(0, 10, 'NAME'), (13, 37, 'DESIGNATION'), (39, 47...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9565217391304348\n",
            "Recall: 0.8825214899713467\n",
            "F1-score: 0.9180327868852459\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       COLLEGE NAME       0.45      0.88      0.60        26\n",
            "COMPANIES WORKED AT       0.70      0.54      0.61        35\n",
            "             DEGREE       0.67      0.90      0.77        31\n",
            "        DESIGNATION       0.60      0.89      0.72        53\n",
            "      EMAIL ADDRESS       0.75      1.00      0.86         6\n",
            "    GRADUATION YEAR       0.94      0.89      0.91        18\n",
            "           LOCATION       0.86      1.00      0.92        12\n",
            "               NAME       0.40      1.00      0.57        12\n",
            "                  O       0.00      0.00      0.00       118\n",
            "             SKILLS       0.92      0.93      0.92       144\n",
            "YEARS OF EXPERIENCE       0.85      0.92      0.88        12\n",
            "\n",
            "           accuracy                           0.66       467\n",
            "          macro avg       0.65      0.81      0.71       467\n",
            "       weighted avg       0.57      0.66      0.60       467\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}